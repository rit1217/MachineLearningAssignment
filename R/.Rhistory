cat('\n'); cat('fold', x, 'starts ....', '\n')
tmp_fit = elmNNRcpp::elm_train(as.matrix(hog[unlist(folds[-x]), ]), y_expand[unlist(folds[-x]), ],
nhid = 100, actfun = 'relu', init_weights = 'uniform_negative',
bias = TRUE, verbose = TRUE)
cat('******************************************', '\n')
tmp_fit
})
str(y_expand)
y = mnist[, ncol(mnist)] + 1
y_expand = elmNNRcpp::onehot_encode(y - 1)
str(y_expand)
fit = lapply(1:length(folds), function(x) {
cat('\n'); cat('fold', x, 'starts ....', '\n')
tmp_fit = elmNNRcpp::elm_train(as.matrix(hog[unlist(folds[-x]), ]),as.matrix(y_expand[unlist(folds[-x])), ],
nhid = 100, actfun = 'relu', init_weights = 'uniform_negative',
bias = TRUE, verbose = TRUE)
cat('******************************************', '\n')
tmp_fit
})
fit = lapply(1:length(folds), function(x) {
cat('\n'); cat('fold', x, 'starts ....', '\n')
tmp_fit = elmNNRcpp::elm_train(as.matrix(hog[unlist(folds[-x]), ]),as.matrix(y_expand[unlist(folds[-x]), ]),
nhid = 100, actfun = 'relu', init_weights = 'uniform_negative',
bias = TRUE, verbose = TRUE)
cat('******************************************', '\n')
tmp_fit
})
library(KernelKnn)
library(elmNNRcpp)
library(OpenImageR)
system("wget https://raw.githubusercontent.com/mlampros/DataSets/master/mnist.zip")
mnist <- read.table(unz("mnist.zip", "mnist.csv"), nrows = 1000, header = T,
quote = "\"", sep = ",")
x = mnist[, -ncol(mnist)]
y = mnist[, ncol(mnist)]
# use the hog-features as input data
#-----------------------------------
hog = OpenImageR::HOG_apply(x, cells = 6, orientations = 9, rows = 28, columns = 28, threads = 6)
y_expand = elmNNRcpp::onehot_encode(y - 1)
str(y_expand)
# 4-fold cross-validation
#------------------------
str(y)
folds = KernelKnn:::class_folds(folds = 4, as.factor(y))
str(folds)
fit = lapply(1:length(folds), function(x) {
cat('\n'); cat('fold', x, 'starts ....', '\n')
tmp_fit = elmNNRcpp::elm_train(as.matrix(hog[unlist(folds[-x]), ]),as.matrix(y_expand[unlist(folds[-x]), ]),
nhid = 100, actfun = 'relu', init_weights = 'uniform_negative',
bias = TRUE, verbose = TRUE)
cat('******************************************', '\n')
tmp_fit
})
# Time difference of 5.698552 mins
str(fit)
# predictions for 4-fold cross validation
#----------------------------------------
test_acc = unlist(lapply(1:length(fit), function(x) {
pr_te = elmNNRcpp::elm_predict(fit[[x]], newdata = as.matrix(hog[folds[[x]], ]))
pr_max_col = max.col(pr_te, ties.method = "random")
y_true = max.col(y_expand[folds[[x]], ])
mean(pr_max_col == y_true)
}))
test_acc
# [1] 0.9825143 0.9848571 0.9824571 0.9822857
cat('Accuracy ( Mnist data ) :', round(mean(test_acc) * 100, 2), '\n')
# Accuracy ( Mnist data ) : 98.3
library(KernelKnn)
library(elmNNRcpp)
library(OpenImageR)
mnist <- read.table(unz("mnist.zip", "mnist.csv"), nrows = 1000, header = T,
quote = "\"", sep = ",")
x = mnist[, -ncol(mnist)]
y = mnist[, ncol(mnist)]
# use the hog-features as input data
#-----------------------------------
hog = OpenImageR::HOG_apply(x, cells = 6, orientations = 9, rows = 28, columns = 28, threads = 6)
y_expand = elmNNRcpp::onehot_encode(y - 1)
str(y_expand)
# 4-fold cross-validation
#------------------------
str(y)
folds = KernelKnn:::class_folds(folds = 4, as.factor(y))
str(folds)
fit = lapply(1:length(folds), function(x) {
cat('\n'); cat('fold', x, 'starts ....', '\n')
tmp_fit = elmNNRcpp::elm_train(as.matrix(hog[unlist(folds[-x]), ]),as.matrix(y_expand[unlist(folds[-x]), ]),
nhid = 100, actfun = 'relu', init_weights = 'uniform_negative',
bias = TRUE, verbose = TRUE)
cat('******************************************', '\n')
tmp_fit
})
# Time difference of 5.698552 mins
str(fit)
# predictions for 4-fold cross validation
#----------------------------------------
test_acc = unlist(lapply(1:length(fit), function(x) {
pr_te = elmNNRcpp::elm_predict(fit[[x]], newdata = as.matrix(hog[folds[[x]], ]))
pr_max_col = max.col(pr_te, ties.method = "random")
y_true = max.col(y_expand[folds[[x]], ])
mean(pr_max_col == y_true)
}))
test_acc
# [1] 0.9825143 0.9848571 0.9824571 0.9822857
cat('Accuracy ( Mnist data ) :', round(mean(test_acc) * 100, 2), '\n')
# Accuracy ( Mnist data ) : 98.3
library(KernelKnn)
library(elmNNRcpp)
library(OpenImageR)
system("wget https://raw.githubusercontent.com/mlampros/DataSets/master/mnist.zip")
mnist <- read.table(unz("mnist.zip", "mnist.csv"), nrows = 1000, header = T,
quote = "\"", sep = ",")
x = mnist[, -ncol(mnist)]
y = mnist[, ncol(mnist)]
# use the hog-features as input data
#-----------------------------------
hog = OpenImageR::HOG_apply(x, cells = 6, orientations = 9, rows = 28, columns = 28, threads = 6)
y_expand = elmNNRcpp::onehot_encode(y - 1)
str(y_expand)
# 4-fold cross-validation
#------------------------
str(y)
folds = KernelKnn:::class_folds(folds = 4, as.factor(y))
str(folds)
fit = lapply(1:length(folds), function(x) {
cat('\n'); cat('fold', x, 'starts ....', '\n')
tmp_fit = elmNNRcpp::elm_train(as.matrix(hog[unlist(folds[-x]), ]),as.matrix(y_expand[unlist(folds[-x]), ]),
nhid = 100, actfun = 'relu', init_weights = 'uniform_negative',
bias = TRUE, verbose = TRUE)
cat('******************************************', '\n')
tmp_fit
})
# Time difference of 5.698552 mins
str(fit)
# predictions for 4-fold cross validation
#----------------------------------------
test_acc = unlist(lapply(1:length(fit), function(x) {
pr_te = elmNNRcpp::elm_predict(fit[[x]], newdata = as.matrix(hog[folds[[x]], ]))
pr_max_col = max.col(pr_te, ties.method = "random")
y_true = max.col(y_expand[folds[[x]], ])
mean(pr_max_col == y_true)
}))
test_acc
# [1] 0.9825143 0.9848571 0.9824571 0.9822857
cat('Accuracy ( Mnist data ) :', round(mean(test_acc) * 100, 2), '\n')
# Accuracy ( Mnist data ) : 98.3
str(y)
str(y-1)
str(y_expand)
a = [1,2,3]
str(mnist)
str(x)
library(KernelKnn)
library(elmNNRcpp)
library(OpenImageR)
mnist <- read.table(unz("mnist.zip", "mnist.csv"), nrows = 1000, header = T,
quote = "\"", sep = ",")
x = mnist[, ncol(mnist)]
y = mnist[, ncol(mnist)]
# use the hog-features as input data
#-----------------------------------
hog = OpenImageR::HOG_apply(x, cells = 6, orientations = 9, rows = 28, columns = 28, threads = 6)
y_expand = elmNNRcpp::onehot_encode(y - 1)
str(y_expand)
# 4-fold cross-validation
#------------------------
str(y)
folds = KernelKnn:::class_folds(folds = 4, as.factor(y))
str(folds)
fit = lapply(1:length(folds), function(x) {
cat('\n'); cat('fold', x, 'starts ....', '\n')
tmp_fit = elmNNRcpp::elm_train(as.matrix(hog[unlist(folds[-x]), ]),as.matrix(y_expand[unlist(folds[-x]), ]),
nhid = 100, actfun = 'relu', init_weights = 'uniform_negative',
bias = TRUE, verbose = TRUE)
cat('******************************************', '\n')
tmp_fit
})
# Time difference of 5.698552 mins
str(fit)
# predictions for 4-fold cross validation
#----------------------------------------
test_acc = unlist(lapply(1:length(fit), function(x) {
pr_te = elmNNRcpp::elm_predict(fit[[x]], newdata = as.matrix(hog[folds[[x]], ]))
pr_max_col = max.col(pr_te, ties.method = "random")
y_true = max.col(y_expand[folds[[x]], ])
mean(pr_max_col == y_true)
}))
test_acc
# [1] 0.9825143 0.9848571 0.9824571 0.9822857
cat('Accuracy ( Mnist data ) :', round(mean(test_acc) * 100, 2), '\n')
# Accuracy ( Mnist data ) : 98.3
mnist <- read.table(unz("mnist.zip", "mnist.csv"), nrows = 1000, header = T,
quote = "\"", sep = ",")
x = mnist[, -ncol(mnist)]
y = mnist[, ncol(mnist)]
x
y
library(elmNNRcpp)
library(OpenImageR)
mnist <- read.table(unz("mnist.zip", "mnist.csv"), nrows = 70000, header = T,
quote = "\"", sep = ",")
#create sampling
rows = sample(nrow(mnist), size = 5000, replace = TRUE)
x = mnist[rows,-ncol(mnist)]
y = mnist[rows, ncol(mnist)]
# use the hog-features as input data
hog = OpenImageR::HOG_apply(x, cells = 6, orientations = 9, rows = 28, columns = 28, threads = 4)
y_expand = elmNNRcpp::onehot_encode(y)
#train elm
model = elmNNRcpp::elm_train(as.matrix(hog[1:5000,]), y_expand,
nhid = 100, actfun = 'relu', init_weights = 'uniform_negative',
bias = TRUE, verbose = TRUE)
#create testing dataset
new_rows = sample(nrow(mnist), size = 5000, replace = TRUE)
x2 = mnist[new_rows,-ncol(mnist)]
y2 = mnist[new_rows, ncol(mnist)]
hog2 = OpenImageR::HOG_apply(x2, cells = 6, orientations = 9, rows = 28, columns = 28, threads = 4)
y_expand2 = elmNNRcpp::onehot_encode(y2)
#predict the dataset
pr_te = elmNNRcpp::elm_predict(model, newdata = as.matrix(hog2))
pr_max_col = max.col(pr_te, ties.method = "random")
y_true = max.col(y_expand2)
test_acc = mean(pr_max_col == y_true)
cat('Accuracy :', test_acc, '\n')
insurance <- read.csv("insurance.csv", stringsAsFactors = TRUE, fileEncoding = "UTF-8-BOM")
str(insurance)
summary(insurance$expenses)
hist(insurance$expenses)
table(insurance$region)
cor(insurance[c("age","bmi","children","expenses")])
pairs(insurance[c("age","bmi","children","expenses")])
install.packages("psych")
library(psych)
pairs.panels(insurance[c("age","bmi","children","expenses")])
ins_model <- lm(expenses ~ age + children + bmi + sex + smoker + region, data = insurance)
ins_model
ins_model1 <- lm(expenses ~ ., data = insurance)
ins_model1
insurance$age2 <- insurance$age^2
str(insurance$age2)
insurance$bmi30 <- ifelse(insurance$bmi >= 30, 1, 0)
ins_model2 <- lm(expenses ~ age + age2 + children + bmi + sex + bmi30*smoker + region, data = insurance)
summary(ins_model2)
insurance$pred <- predict(ins_model2, insurance)
cor(insurance$pred, insurance$expenses)
plot(insurance$pred, insurance$expenses)
abline(a=0, b=1, col = "red", lwd =3, lty = 2)
launch <- read.csv("challenger.csv", fileEncoding="UTF-8-BOM")
str(launch)
b <- cov(launch$temperature, launch$distress_ct) / var(launch$temperature)
a <- mean(launch$distress_ct) - b*mean(launch$temperature)
#correlation
r <- cov(launch$temperature, launch$distress_ct) / (sd(launch$temperature)*sd(launch$distress_ct))
r
cor(launch$temperature, launch$distress_ct)
#regression
reg <- function( y, x) {
x <- as.matrix(x)
x <- cbind(Intercept = 1, x)
# %*% = matrix multiplication
# t(x) = X transpose
b <- solve(t(x) %*% x) %*% t(x) %*% y #(XtransposeX)invert XtransposeY
colnames(b) <- "estimate"
print(b)
}
str(launch)
reg(y = launch$distress_ct, x = launch[2]) #simple linear regression x = temperature
# beta0 = 3.70
# beta1 = -0.048
reg(y = launch$distress_ct, x = launch[2:4]) #multiple linear regression
wine <- read.csv("whitewines.csv")
str(wine)
hist(wine$quality)
summary(wine)
wine_train <- wine[1:3750,]
wine_test <- wine[3751:4898,]
install.packages("rpart")
library(rpart)
m.rpart <- rpart(quality ~ ., data = wine_train)
m.rpart
summary(m.rpart)
install.packages("rpart.plot")
wine <- read.csv("whitewines.csv")
str(wine)
hist(wine$quality)
summary(wine)
wine_train <- wine[1:3750,]
wine_test <- wine[3751:4898,]
install.packages("rpart")
library(rpart)
m.rpart <- rpart(quality ~ ., data = wine_train)
m.rpart
summary(m.rpart)
install.packages("rpart.plot")
library(rpart.plot)
rpart.plot(m.rpart, digits = 3)
rpart.plot(m.rpart, digits = 4, fallen.leaves = TRUE, type = 3, extra = 101)
p.rpart <- predict(m.rpart, wine_test)
summary(p.rpart)
summary(wine_test$quality)
cor(p.rpart, wine_test$quality)
MAE <- function(actual, predicted) {
mean(abs(actual-predicted))
}
MAE(wine_test$quality, p.rpart)
mean(wine_train$quality)
install.packages("rpart")
library(elmNNRcpp)
library(OpenImageR)
mnist <- read.table(unz("mnist.zip", "mnist.csv"), nrows = 70000, header = T,
quote = "\"", sep = ",")
#create sampling
rows = sample(nrow(mnist), size = 5000, replace = TRUE)
x = mnist[rows,-ncol(mnist)]
y = mnist[rows, ncol(mnist)]
# use the hog-features as input data
hog = OpenImageR::HOG_apply(x, cells = 6, orientations = 9, rows = 28, columns = 28, threads = 4)
y_expand = elmNNRcpp::onehot_encode(y)
#train elm
model = elmNNRcpp::elm_train(as.matrix(hog[1:5000,]), y_expand,
nhid = 100, actfun = 'relu', init_weights = 'uniform_negative',
bias = TRUE, verbose = TRUE)
#create testing dataset
new_rows = sample(nrow(mnist), size = 5000, replace = TRUE)
x2 = mnist[new_rows,-ncol(mnist)]
y2 = mnist[new_rows, ncol(mnist)]
hog2 = OpenImageR::HOG_apply(x2, cells = 6, orientations = 9, rows = 28, columns = 28, threads = 4)
y_expand2 = elmNNRcpp::onehot_encode(y2)
#predict the dataset
pr_te = elmNNRcpp::elm_predict(model, newdata = as.matrix(hog2))
pr_max_col = max.col(pr_te, ties.method = "random")
y_true = max.col(y_expand2)
test_acc = mean(pr_max_col == y_true)
cat('Accuracy :', test_acc, '\n')
library(elmNNRcpp)
library(OpenImageR)
mnist <- read.table(unz("mnist.zip", "mnist.csv"), nrows = 70000, header = T,
quote = "\"", sep = ",")
#create sampling
rows = sample(nrow(mnist), size = 100, replace = TRUE)
x = mnist[rows,-ncol(mnist)]
y = mnist[rows, ncol(mnist)]
# use the hog-features as input data
hog = OpenImageR::HOG_apply(x, cells = 6, orientations = 9, rows = 28, columns = 28, threads = 4)
y_expand = elmNNRcpp::onehot_encode(y)
#train elm
model = elmNNRcpp::elm_train(as.matrix(hog[1:5000,]), y_expand,
nhid = 100, actfun = 'relu', init_weights = 'uniform_negative',
bias = TRUE, verbose = TRUE)
#create testing dataset
new_rows = sample(nrow(mnist), size = 5000, replace = TRUE)
x2 = mnist[new_rows,-ncol(mnist)]
y2 = mnist[new_rows, ncol(mnist)]
hog2 = OpenImageR::HOG_apply(x2, cells = 6, orientations = 9, rows = 28, columns = 28, threads = 4)
y_expand2 = elmNNRcpp::onehot_encode(y2)
#predict the dataset
pr_te = elmNNRcpp::elm_predict(model, newdata = as.matrix(hog2))
pr_max_col = max.col(pr_te, ties.method = "random")
y_true = max.col(y_expand2)
test_acc = mean(pr_max_col == y_true)
cat('Accuracy :', test_acc, '\n')
mnist <- read.table(unz("mnist.zip", "mnist.csv"), nrows = 70000, header = T,
quote = "\"", sep = ",")
rows = sample(nrow(mnist), size = 5000, replace = TRUE)
x = mnist[rows,-ncol(mnist)]
y = mnist[rows, ncol(mnist)]
# use the hog-features as input data
hog = OpenImageR::HOG_apply(x, cells = 6, orientations = 9, rows = 28, columns = 28, threads = 4)
y_expand = elmNNRcpp::onehot_encode(y)
str(y_expand)
#train elm
model = elmNNRcpp::elm_train(as.matrix(hog[1:5000,]), y_expand,
nhid = 100, actfun = 'relu', init_weights = 'uniform_negative',
bias = TRUE, verbose = TRUE)
new_rows = sample(nrow(mnist), size = 5000, replace = TRUE)
x2 = mnist[new_rows,-ncol(mnist)]
y2 = mnist[new_rows, ncol(mnist)]
hog2 = OpenImageR::HOG_apply(x2, cells = 6, orientations = 9, rows = 28, columns = 28, threads = 4)
y_expand2 = elmNNRcpp::onehot_encode(y2)
#predict the dataset
pr_te = elmNNRcpp::elm_predict(model, newdata = as.matrix(hog2))
pr_te
pr_max_col = max.col(pr_te, ties.method = "random")
y_true = max.col(y_expand2)
test_acc = mean(pr_max_col == y_true)
cat('Accuracy :', test_acc, '\n')
letter <- read.csv("letterdata.csv", stringsAsFactors = TRUE)
str(letter)
letters_train <- letter[1:16000,]
letters_test <- letter[16000:20000,]
install.packages("kernlab")
library(kernlab)
letter_classifier <- ksvm(letter ~ ., data = letter_train, kernel = "vanilladot")
letter_classifier <- ksvm(letter ~ ., data = letters_train, kernel = "vanilladot")
letter_classifier
letter_predictions <- predict(leter_classifier, letters_test)
letter_predictions <- predict(letter_classifier, letters_test)
head(letter_predictions)
table(letter_predictions, letters_test$letter)
agreement <- letter_predictions == letters_test$letter
table(agreement)
prop.table(table(agreement))
letter_classifier_rbf <- ksvm(letter ~ ., data = letters_train, kernel = "rbfdot")
letter_predictions_rbf <- predict(letter_classifier_rbf, letters_test)
agreement_rbf <- letter_predictions_rbf == ltters_test$letter
table(agreement_rbf)
agreement_rbf <- letter_predictions_rbf == letters_test$letter
table(agreement_rbf)
prop.table(agreement_rbf)
table(agreement_rbf)
prop.table(table(agreement_rbf))
cost_values <- c(1, seq(from = 5, to = 40, by = 5))
cost_values <- c(1, seq(from = 5, to = 40, by = 5))
accuracy_values <- sapply(cost_values, function(x) {
set.seed(12345)
m <- ksvm(letter ~ ., data = letters_train, kernel = "rbfdot", C= x)
pred <- predict(m, letters_test)
agree <- ifelse(pred == letters_test$letter, 1, 0)
accuracy <- sum(agree)/nrow(letters_test)
return (accuracy)
})
plot(cost_values, accuracy_values, type = "b")
# Packages
library(xgboost)
library(magrittr)
library(dplyr)
library(Matrix)
library(caret)
# Data
data <- read.csv("binary.csv", header = T)
data$rank <- as.factor(data$rank)
data
# Data
data <- read.csv("binary.csv", header = T)
data
# Packages
library(xgboost)
library(magrittr)
library(dplyr)
library(Matrix)
library(caret)
# Data
data <- read.csv("binary.csv", header = T)
data$rank <- as.factor(data$rank)
# Partition data
set.seed(1234)
ind <- sample(2, nrow(data), replace = T, prob = c(0.8, 0.2))
train <- data[ind==1,]
test <- data[ind==2,]
# Create matrix
trainm <- sparse.model.matrix(admit ~ .-1, data = train)
head(trainm)
groceries <- read.csv("groceries.csv", header = FALSE)
View(groceries)
install.packages("arules")
library(arules)
groceries <- read.transactions("groceries.csv", sep=",")
summary(groceries)
inspect(groceries[1:5])
itemFrequency( groceries[, 1:3])
itemFrequencyPlot(groceries, support = 0.1)
itemFrequencyPlot(groceries, topN = 20)
image(groceries[1:5])
image(sample(groceries,100))
apriori(groceries)
groceryrules <- apriori(groceries, parameter = list(support = 0.006, confidence = 0.25, minlen = 2))
groceryrules
summary(groceryrules)
inspect(groceryrules[1:3])
inspect(sort(groceryrules, by = "lift")[1:5])
berryrules <- subset(groceryrules, items %in% "berries")
inspect(berryrules)
write(groceryrules, file = "groceryrules.csv", sep = ",", quote = TRUE, row.names = FALSE)
groceryrules_df <- as(groceryrules, "data.frane")
groceryrules_df <- as(groceryrules, "data.frame")
str(groceryrules_df)
plot(groceryrules)
install.packages("arulesViz")
library(arulesViz)
plot(groceryrules)
plot(groceryrules, method = "grouped")
Subrules <- sample(groceryrules, 50)
plot(Subrules, method = "graph")
library(caret)
test <- read.csv("test.csv", stringsAsFactors = TRUE)
train <- read.csv("train.csv", stringsAsFactors = TRUE)
train_data <- sapply( train, as.numeric)
test_data <- sapply( test, as.numeric)
#XG Boosting Regression
grid_tune <- expand.grid(
nrounds = 1500, #number of trees
max_depth = 4, #maximum depth of the tree
eta = 0.025, #learning rate
gamma =  0.1, #tuned gamma(pruning)
colsample_bytree = 1, #subsampling the ratio of columns
min_child_weight = 1, #stop term
subsample = 0.5
)
#Cross validation as resampling method
train_control <- trainControl(method = "cv", #cross validation method
number = 3, #split the data into 3 groups
verboseIter = TRUE, #print information of each iteration out
allowParallel = TRUE )#allow parallel processing
xgb_tune <- train(x = train_data[,-81],
y= train_data[,81], #SalePrice
trControl = train_control,
tuneGrid = grid_tune,
method = "xgbTree",
verbose = TRUE)
pred <- predict(xgb_tune, test_data)
df <- data.frame(Id = test$Id,SalePrice = pred)
write.csv(df,"./HousePricePredicted.csv", row.names = FALSE)
